 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Document Classification using EfficientNet\n",
    "\n",
    "This notebook demonstrates how to leverage transfer learning with EfficientNet-B0 for identity document classification.\n",
    "\n",
    "EfficientNet is a family of convolutional neural networks that achieves state-of-the-art accuracy with significantly fewer parameters than traditional models. It uses a technique called compound scaling to scale the network's depth, width, and resolution in a principled way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# Add current directory to path to import local modules\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import custom modules\n",
    "from dataset import load_data, IDDocumentDataset, get_transforms\n",
    "from models import create_efficient_net_b0\n",
    "from train import train_model, evaluate_model, visualize_training_history, model_summary, visualize_misclassified_samples\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Let's configure our training parameters. For EfficientNet, we'll use slightly different hyperparameters optimized for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data parameters\n",
    "DATA_DIR = '../../data/cropped_images'  # Path to dataset directory\n",
    "IMG_SIZE = 224  # Input image size (EfficientNet-B0 was trained on 224px images)\n",
    "VAL_SPLIT = 0.15  # Percentage of data to use for validation\n",
    "TEST_SPLIT = 0.15  # Percentage of data to use for testing\n",
    "BATCH_SIZE = 32  # Batch size for training - adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # Number of workers for data loading\n",
    "\n",
    "# Training parameters - tuned for transfer learning\n",
    "EPOCHS = 30  # Maximum number of epochs to train for\n",
    "LEARNING_RATE = 3e-4  # Learning rate for transfer learning\n",
    "WEIGHT_DECAY = 1e-5  # L2 regularization\n",
    "PATIENCE = 7  # Early stopping patience\n",
    "CHECKPOINT_DIR = Path('../../checkpoints/efficientnet')  # Directory to save model checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)  # Create checkpoint directory if it doesn't exist\n",
    "\n",
    "# Transfer learning settings\n",
    "PRETRAINED = True  # Use pretrained weights\n",
    "FREEZE_BACKBONE = False  # Whether to freeze the backbone layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration\n",
    "\n",
    "Let's load our dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data\n",
    "data = load_data(\n",
    "    data_dir=DATA_DIR,\n",
    "    img_size=IMG_SIZE,\n",
    "    val_split=VAL_SPLIT,\n",
    "    test_split=TEST_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = data['class_names']\n",
    "num_classes = data['num_classes']\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Visualization\n",
    "\n",
    "Let's visualize the original images from our dataset to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_original_samples(dataset, class_names, num_images=5, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize original samples from a dataset without any transformations.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch dataset object\n",
    "        class_names (list): List of class names\n",
    "        num_images (int): Number of images to display\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "    # Select random indices\n",
    "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image path and label directly from dataset\n",
    "        img_path = dataset.image_paths[idx]\n",
    "        label = dataset.labels[idx]\n",
    "        \n",
    "        # Load original image without transformations\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Class: {class_names[label]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize original samples from each class\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"Class: {class_name}\")\n",
    "    # Filter dataset to show only samples from this class\n",
    "    class_indices = [j for j, label in enumerate(data['train_dataset'].labels) if label == i]\n",
    "    \n",
    "    # Choose a subset of indices\n",
    "    subset_indices = np.random.choice(class_indices, min(5, len(class_indices)), replace=False)\n",
    "    \n",
    "    # Create a temporary dataset with just these samples\n",
    "    temp_dataset = IDDocumentDataset(\n",
    "        [data['train_dataset'].image_paths[j] for j in subset_indices],\n",
    "        [data['train_dataset'].labels[j] for j in subset_indices],\n",
    "        None,\n",
    "        'train'\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_original_samples(temp_dataset, class_names)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating and Configuring the EfficientNet Model\n",
    "\n",
    "Now we'll create our EfficientNet-B0 model with pretrained weights and configure it for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create the EfficientNet model\n",
    "model = create_efficient_net_b0(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=PRETRAINED,\n",
    "    freeze_backbone=FREEZE_BACKBONE\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "model_summary(model)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Up Training Components\n",
    "\n",
    "Let's set up our loss function, optimizer, and learning rate scheduler. For transfer learning, we'll use a OneCycleLR scheduler which has been shown to work well with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer - Adam with weight decay for regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "# We'll use OneCycleLR which works well for transfer learning\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(data['train_loader']),\n",
    "    pct_start=0.3,  # Spend 30% of time warming up\n",
    "    div_factor=25,  # LR starts at max_lr/25\n",
    "    final_div_factor=10000,  # Final LR is max_lr/10000\n",
    "    anneal_strategy='cos'  # Cosine annealing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Now we'll train our EfficientNet model. The training loop includes both training and validation phases, with early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a unique model name with timestamp\n",
    "model_name = f\"efficientnet_b0_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(\n",
    "    model=model,\n",
    "    dataloaders={\n",
    "        'train': data['train_loader'],\n",
    "        'val': data['val_loader']\n",
    "    },\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=EPOCHS,\n",
    "    device=device,\n",
    "    save_dir=CHECKPOINT_DIR,\n",
    "    model_name=model_name,\n",
    "    early_stopping_patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Training Results\n",
    "\n",
    "Let's visualize our training history to see how the model performed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize training history\n",
    "visualize_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Now let's evaluate our trained model on the test set to see how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set\n",
    "test_metrics = evaluate_model(model, data['test_loader'], device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyzing Misclassifications\n",
    "\n",
    "Let's look at examples that our model misclassified to understand its weaknesses and potential areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize misclassified samples\n",
    "visualize_misclassified_samples(model, data['test_loader'], class_names, device, num_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference on Single Image\n",
    "\n",
    "Let's use our trained model to classify individual images and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_single_image(model, image_path, transform, class_names, device):\n",
    "    \"\"\"\n",
    "    Predict class for a single image and visualize the result.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        image_path: Path to the image file\n",
    "        transform: Transformation pipeline for inference\n",
    "        class_names: List of class names\n",
    "        device: Device to run inference on\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image']\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        values, indices = torch.topk(probabilities, 3)  # Get top 3 predictions\n",
    "    \n",
    "    # Display prediction results\n",
    "    values = values.squeeze().cpu().numpy() * 100  # Convert to percentage\n",
    "    indices = indices.squeeze().cpu().numpy()\n",
    "    \n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in range(min(3, len(class_names))):\n",
    "        print(f\"{class_names[indices[i]]}: {values[i]:.2f}%\")\n",
    "    \n",
    "    # Create a horizontal bar chart for visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(y=[class_names[idx] for idx in indices[:3]], width=values[:3])\n",
    "    plt.xlabel('Confidence (%)')\n",
    "    plt.title('Top 3 Predictions')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.gca().invert_yaxis()  # Highest confidence at the top\n",
    "    plt.show()\n",
    "\n",
    "# Get test images for inference\n",
    "test_dataset = data['test_dataset']\n",
    "test_indices = np.random.choice(len(test_dataset), 3, replace=False)\n",
    "\n",
    "# Use validation/test transform for inference (no augmentation)\n",
    "inference_transform = get_transforms(img_size=IMG_SIZE)['test']\n",
    "\n",
    "# Run inference on multiple test images\n",
    "for idx in test_indices:\n",
    "    test_image_path = test_dataset.image_paths[idx]\n",
    "    test_label = test_dataset.labels[idx]\n",
    "    print(f\"\\nTrue class: {class_names[test_label]}\")\n",
    "    predict_single_image(model, test_image_path, inference_transform, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saving the Model\n",
    "\n",
    "Let's save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the model\n",
    "save_path = os.path.join(CHECKPOINT_DIR, f\"{model_name}_final.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': class_names,\n",
    "    'num_classes': num_classes,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'metrics': test_metrics,\n",
    "    'model_type': 'efficientnet'\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Loading and Using the Model for Inference\n",
    "\n",
    "Let's demonstrate how to load the saved model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_efficientnet_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Load a saved EfficientNet model and return it with associated metadata.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model file\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        model: The loaded PyTorch model\n",
    "        metadata: Dictionary containing model metadata\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Extract metadata\n",
    "    class_names = checkpoint.get('class_names')\n",
    "    num_classes = checkpoint.get('num_classes')\n",
    "    img_size = checkpoint.get('img_size', 224)\n",
    "    metrics = checkpoint.get('metrics', {})\n",
    "    \n",
    "    # Create model\n",
    "    model = create_efficient_net_b0(num_classes=num_classes, pretrained=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, {\n",
    "        'class_names': class_names,\n",
    "        'num_classes': num_classes,\n",
    "        'img_size': img_size,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "# Test loading the model\n",
    "loaded_model, metadata = load_efficientnet_model(save_path, device)\n",
    "print(f\"Model loaded successfully with {metadata['num_classes']} classes\")\n",
    "print(f\"Test metrics: Accuracy={metadata['metrics'].get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "# Test inference with the loaded model\n",
    "test_idx = np.random.randint(0, len(test_dataset))\n",
    "test_image_path = test_dataset.image_paths[test_idx]\n",
    "test_label = test_dataset.labels[test_idx]\n",
    "\n",
    "print(f\"\\nTrue class: {metadata['class_names'][test_label]}\")\n",
    "predict_single_image(loaded_model, test_image_path, inference_transform, metadata['class_names'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Experimenting with Freezing Layers\n",
    "\n",
    "Let's experiment with freezing the backbone layers of EfficientNet to see how it affects performance. This is a common technique in transfer learning to prevent overfitting when working with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare the number of trainable parameters with and without freezing\n",
    "model_unfrozen = create_efficient_net_b0(num_classes=num_classes, pretrained=True, freeze_backbone=False)\n",
    "model_frozen = create_efficient_net_b0(num_classes=num_classes, pretrained=True, freeze_backbone=True)\n",
    "\n",
    "trainable_unfrozen = sum(p.numel() for p in model_unfrozen.parameters() if p.requires_grad)\n",
    "trainable_frozen = sum(p.numel() for p in model_frozen.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters (unfrozen): {trainable_unfrozen:,}\")\n",
    "print(f\"Trainable parameters (frozen): {trainable_frozen:,}\")\n",
    "print(f\"Reduction: {(1 - trainable_frozen/trainable_unfrozen):.2%}\")\n",
    "\n",
    "# The notebook implementation stops here, but in a real scenario you would:\n",
    "# 1. Train the model with frozen layers using a smaller learning rate\n",
    "# 2. Compare the performance with the unfrozen model\n",
    "# 3. Potentially unfreeze layers gradually for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "In this notebook, we've walked through the process of using EfficientNet-B0 with transfer learning for identity document classification:\n",
    "\n",
    "1. We set up our environment and loaded our dataset\n",
    "2. We explored and visualized the dataset\n",
    "3. We created an EfficientNet-B0 model with pretrained weights\n",
    "4. We configured the model for our classification task\n",
    "5. We used advanced training techniques like OneCycleLR scheduling\n",
    "6. We trained, evaluated, and analyzed the model\n",
    "7. We performed inference on test images\n",
    "8. We saved and loaded the model for future use\n",
    "9. We explored the effects of freezing backbone layers for transfer learning\n",
    "\n",
    "EfficientNet is a powerful model for this task because it efficiently extracts features from images. The pretrained weights help the model leverage general image understanding to quickly adapt to our specific document classification task, even with a relatively small dataset.\n",
    "\n",
    "For further improvements, you could:\n",
    "1. Try other EfficientNet variants (B1-B7) for potentially better performance\n",
    "2. Experiment with more advanced data augmentation techniques\n",
    "3. Implement gradual unfreezing during training\n",
    "4. Try different learning rate schedules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}